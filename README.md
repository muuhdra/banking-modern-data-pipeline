# ğŸ¦ Banking Modern Data Stack


## ğŸ“Œ Project Overview
This project demonstrates an **end-to-end modern data stack pipeline** for a **Banking domain**.  
We simulate **customer, account, and transaction data**, stream changes in real time, transform them into analytics-ready models, and visualize insights â€” following **best practices of CI/CD and data warehousing**.

ğŸ‘‰ Think of it as a **real-world banking data ecosystem** built on modern data tools.  

---

## ğŸ—ï¸ Architecture  



**Pipeline Flow:**
1. **Data Generator** â†’ Simulates banking transactions, accounts & customers (via Faker).  
2. **Kafka + Debezium** â†’ Streams change data (CDC) into MinIO (S3-compatible storage).  
3. **Airflow** â†’ Orchestrates data ingestion & snapshots into Snowflake.  
4. **Snowflake** â†’ Cloud Data Warehouse (Bronze â†’ Silver â†’ Gold).  
5. **DBT** â†’ Applies transformations, builds marts & snapshots (SCD Type-2).  
6. **CI/CD with GitHub Actions** â†’ Automated tests, build & deployment.  

---

## Tech Stack
- **Snowflake** â†’ Cloud Data Warehouse  
- **DBT** â†’ Transformations, testing, snapshots (SCD Type-2)  
- **Apache Airflow** â†’ Orchestration & DAG scheduling  
- **Apache Kafka + Debezium** â†’ Real-time streaming & CDC  
- **MinIO** â†’ S3-compatible object storage  
- **Postgres** â†’ Source OLTP system  
- **Python (Faker)** â†’ Data simulation  
- **Docker & docker-compose** â†’ Containerized setup  
- **Git & GitHub Actions** â†’ CI/CD workflows  

---

## Key Features
- **PostgreSQL OLTP**: Source relational database with ACID guarantees (customers, accounts, transactions)  
- **Simulated banking system**: customers, accounts, and transactions  
- **Change Data Capture (CDC)** via Kafka + Debezium (capturing Postgres WAL)  
- **Raw â†’ Staging â†’ Fact/Dimension** models in DBT  
- **Snapshots for history tracking** (slowly changing dimensions)  
- **Automated pipeline orchestration** using Airflow  
- **CI/CD pipeline** with dbt tests + GitHub Actions  

---

## Repository Structure
```text
banking-modern-datastack/
â”œâ”€â”€ .github/workflows/         # CI/CD pipelines (ci.yml, cd.yml)
â”œâ”€â”€ banking_dbt/              # DBT project
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/           # Staging models
â”‚   â”‚   â”œâ”€â”€ marts/             # Facts & dimensions
â”‚   â”‚   â””â”€â”€ sources.yml
â”‚   â”œâ”€â”€ snapshots/             # SCD2 snapshots
â”‚   â””â”€â”€ dbt_project.yml
â”œâ”€â”€ consumer
â”‚   â””â”€â”€ kafka_to_minio.py
â”œâ”€â”€ data-generator/            # Faker-based data simulator
â”‚   â””â”€â”€ faker_generator.py
â”œâ”€â”€ docker/                    # Airflow DAGs, plugins, etc.
â”‚   â”œâ”€â”€ dags/                  # DAGs (minio_to_snowflake, scd_snapshots)
â”œâ”€â”€ kafka-debezium/            # Kafka connectors & CDC logic
â”‚   â””â”€â”€ generate_and_post_connector.py
â”œâ”€â”€ postgres/                  # Postgres schema (OLTP DDL & seeds)
â”‚   â””â”€â”€ schema.sql
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml         # Containerized infra
â”œâ”€â”€ dockerfile-airflow.dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## Step-by-Step Implementation  

### **1. Data Simulation**  
- Generated synthetic banking data (**customers, accounts, transactions**) using **Faker**.  
- Inserted data into **PostgreSQL (OLTP)** so the system behaves like a real transactional database (**ACID, constraints**).  
- Controlled generation via `config.yaml`.  

---

### **2. Kafka + Debezium CDC**  
- Set up **Kafka Connect & Debezium** to capture changes from **Postgres**.  
- Streamed **CDC events** into **MinIO**.  

---

### **3. Airflow Orchestration**  
- Built DAGs to:  
  - Ingest **MinIO data â†’ Snowflake (Bronze)**.  
  - Schedule **snapshots & incremental loads**.  

---

### **4. Snowflake Warehouse**  
- Organized into **Bronze â†’ Silver â†’ Gold layers**.  
- Created **staging schemas** for ingestion.  

---

### **5. DBT Transformations**  
- **Staging models** â†’ cleaned source data.  
- **Dimension & fact models** â†’ built marts.  
- **Snapshots** â†’ tracked history of accounts & customers.  

---

### **6. CI/CD with GitHub Actions**  
- **ci.yml** â†’ Lint, dbt compile, run tests.  
- **cd.yml** â†’ Deploy DAGs & dbt models on merge.  

---

## Final Deliverables  
- **Automated CDC pipeline** from Postgres â†’ Snowflake  
- **DBT models** (facts, dimensions, snapshots)  
- **Orchestrated DAGs in Airflow**  
- **Synthetic banking dataset** for demos  
- **CI/CD workflows** ensuring reliability  

---

## ğŸŒŸ About Me

Hi! I'm **Jordi Dangoh**. Iâ€™m a Data Engineer, ready for new challenges. My goal is to improve myself, make complex/simple projects and give
the best practice in the role of a data engineer. I don't have many years of experience, but my motivation and curiosity drive me to learn fast,
adapt quickly, and contribute with impactful solutions.

**Author**: *Jaya Chandra Kadiveti*  
**LinkedIn**: [jayachandrakadiveti](https://www.linkedin.com/in/jayachandrakadiveti/)  
**Contact**: [Kadivetijayachandra@gmail.com](mailto:Kadivetijayachandra@gmail.com)  
